{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schätzung des ATTE mittels DoubleML unter Verwendung von Daten aus der Simulationstudie von Lee et al. (2011) und schauen, welche Auswirkungen die Discarding und Truncation Strategie auf die Schätzleistung von DoubleML haben. Dabei wird lineare Regression zur Vorhersage der Outcomes verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notwendige Bibliotheken importieren\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from doubleml import DoubleMLIRM, DoubleMLData\n",
    "from doubleml.utils.resampling import DoubleMLResampling\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import  LinearRegression, LogisticRegressionCV \n",
    "from sklearn.ensemble import  RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.base import clone\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys, os\n",
    "# hier den Pfad zu der Funktion pscore_discard aus functions.py einfügen\n",
    "sys.path.append(os.path.abspath(\".\"))\n",
    "from functions import pscore_discard\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DGP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur Erstellung einer kontinuierlichen Zufallsvariable, die mit der Variable x korreliert ist\n",
    "def sample_cor(x, rho): \n",
    "    y = (rho * (x - np.mean(x))) / np.sqrt(np.var(x))+ np.sqrt(1 - rho**2) * np.random.randn(len(x))\n",
    "    return y \n",
    "\n",
    "# Funktion zur Erstellung von Simulationsdaten\n",
    "def generate_data(size, scenario):\n",
    "    w1 = np.random.randn(size) #mean=0, sd=1\n",
    "    w2 = np.random.randn(size)\n",
    "    w3 = np.random.randn(size)\n",
    "    w4 = np.random.randn(size)\n",
    "    w5 = sample_cor(w1, 0.2)\n",
    "    w6 = sample_cor(w2, 0.9)\n",
    "    w7 = np.random.randn(size)\n",
    "    w8 = sample_cor(w3, 0.2)\n",
    "    w9 = sample_cor(w4, 0.9)\n",
    "    w10 = np.random.randn(size)\n",
    "\n",
    "    # Konvertieren der kontinuierlichen Variablen in binäre Variablen\n",
    "    w1 = (w1 > np.mean(w1)).astype(int)\n",
    "    w3 = (w3 > np.mean(w3)).astype(int)\n",
    "    w5 = (w5 > np.mean(w5)).astype(int)\n",
    "    w6 = (w6 > np.mean(w6)).astype(int)\n",
    "    w8 = (w8 > np.mean(w8)).astype(int)\n",
    "    w9 = (w9 > np.mean(w9)).astype(int)\n",
    "\n",
    "    # Globale Koeffizienten definieren\n",
    "    b0, b1, b2, b3, b4, b5, b6, b7 = 0, 0.8, -0.25, 0.6, -0.4, -0.8, -0.5, 0.7\n",
    "    a0, a1, a2, a3, a4, a5, a6, a7, g1 = -3.85, 0.3, -0.36, -0.73, -0.2, 0.71, -0.19, 0.26, -0.4\n",
    "\n",
    "    # Scenarien für die Erstellung der wahren Propensity Scores definieren\n",
    "    if scenario == \"A\": #model with additivity and linearity\n",
    "        z_a_trueps = 1 / (1 + np.exp(-(b0 + b1*w1 + b2*w2 + b3*w3 + b4*w4 + b5*w5 + b6*w6 + b7*w7)))\n",
    "    elif scenario == \"E\": #mild non-additivity and non-linearity\n",
    "        z_a_trueps = 1 / (1 + np.exp(-(b0 + b1*w1 + b2*w2 + b3*w3 + b4*w4 + b5*w5 + b6*w6 + b7*w7 + b2*w2*w2 + b1*0.5*w1*w3 + b2*0.7*w2*w4 + b4*0.5*w4*w5 + b5*0.5*w5*w6)))\n",
    "    else:  # scenario G - moderate non-additivity and non-linearity\n",
    "        z_a_trueps = 1 / (1 + np.exp(-(b0 + b1*w1 + b2*w2 + b3*w3 + b4*w4 + b5*w5 + b6*w6 + b7*w7 + b2*w2*w2 + b4*w4*w4 + b7*w7*w7 + b1*0.5*w1*w3 + b2*0.7*w2*w4 + b3*0.5*w3*w5 + b4*0.7*w4*w6 + b5*0.5*w5*w7 + b1*0.5*w1*w6 + b2*0.7*w2*w3 + b3*0.5*w3*w4 + b4*0.5*w4*w5 + b5*0.5*w5*w6)))\n",
    "\n",
    "    # Wahrscheinlichkeit von Behandlungszuweisung\n",
    "    prob_exposure = np.random.rand(size)\n",
    "    z_a = np.where(z_a_trueps > prob_exposure, 1, 0)\n",
    "\n",
    "    # Outcome-Variable modellieren und Zufallsrauschen hinzufügen\n",
    "    y_a = a0 + a1*w1 + a2*w2 + a3*w3 + a4*w4 + a5*w8 + a6*w9 + a7*w10 + g1*z_a + np.random.randn(size)\n",
    "\n",
    "    # Simulationsdaten erstellen\n",
    "    sim_data = pd.DataFrame({\n",
    "        'w1': w1, 'w2': w2, 'w3': w3, 'w4': w4, 'w5': w5, \n",
    "        'w6': w6, 'w7': w7, 'w8': w8, 'w9': w9, 'w10': w10,\n",
    "        'z_a': z_a, 'y_a': y_a, 'z_a_trueps': z_a_trueps\n",
    "    })\n",
    "    return sim_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed für die Reproduzierbarkeit festlegen\n",
    "np.random.seed(1234)\n",
    "# Schätzen die Propensity Score und den ATTE für verschiedene Schwellenwerte, Scenarien und Machine Learning-Algorithmen\n",
    "def run_trunc (i, ml_g, ml_m, scenario):\n",
    "    # Liste für die Ergebnisse erstellen\n",
    "    results_tr = []\n",
    "    # Seed festlegen\n",
    "    np.random.seed(i)\n",
    "    # Simulationsdaten erstellen\n",
    "    simdata = generate_data(500, scenario)\n",
    "    # wahren Propensity Score speichern\n",
    "    trueps = simdata['z_a_trueps']\n",
    "    # DoubleMLData Objekt erstellen\n",
    "    data = DoubleMLData(simdata, y_col='y_a', d_cols='z_a', x_cols=['w1', 'w2', 'w3', 'w4', 'w5', 'w6', 'w7', 'w8', 'w9', 'w10'])          \n",
    "    for trim_value in [0.001, 0.01, 0.05, 0.1]:\n",
    "        # DoubleML Objekt erstellen\n",
    "        dml_obj = DoubleMLIRM(data, ml_g, ml_m, trimming_threshold=trim_value, score=\"ATTE\")\n",
    "        # Schätzen der Effekte\n",
    "        dml_obj.fit()\n",
    "        # geschätzte Propensity Score speichern\n",
    "        predicted_ps = dml_obj.predictions['ml_m'].flatten()\n",
    "        dml_summary = dml_obj.summary\n",
    "        dml_summary['scenario'] = scenario\n",
    "        dml_summary['trim_value'] = trim_value\n",
    "        dml_summary['learner'] = str(ml_m)\n",
    "        # Anteil der trunkierten Beobachtungen und Anteil der behandelten Beobachtungen berechnen\n",
    "        dml_summary['share_trimmed_top'] = (((predicted_ps == (1-trim_value)).sum()) / len(simdata))*100\n",
    "        dml_summary['share_trimmed_bottom'] = (((predicted_ps == trim_value).sum()) / len(simdata))*100\n",
    "        dml_summary['share_treated'] = (((simdata.loc[simdata['z_a'] == 1]).sum()) / len(simdata))*100\n",
    "        # Anteil der trunkierten Beobachtungen mit wahren PS   \n",
    "        dml_summary['share_trim_orcl_top'] = ((np.where(trueps >= (1-trim_value), 1, 0).sum())/trueps.shape[0])*100\n",
    "        dml_summary['share_trim_orcl_bottom'] = ((np.where(trueps <= trim_value, 1, 0).sum())/trueps.shape[0])*100 \n",
    "        \n",
    "        # Nuisance Loss berechnen\n",
    "        dml_summary['loss_ml_g0'] = dml_obj.nuisance_loss['ml_g0'][0][0]\n",
    "        dml_summary['loss_ml_g1'] = dml_obj.nuisance_loss['ml_g1'][0][0]\n",
    "        dml_summary['loss_ml_m'] = dml_obj.nuisance_loss['ml_m'][0][0]\n",
    "\n",
    "        # Daten\n",
    "        true_data_z_a_0 = simdata[simdata['z_a'] == 0]\n",
    "        true_data_z_a_1 = simdata[simdata['z_a'] == 1]\n",
    "        pred_g0 = dml_obj.predictions['ml_g0'].flatten()\n",
    "        pred_g1 = dml_obj.predictions['ml_g1'].flatten()\n",
    "        \n",
    "        # RMSE für das Outcome Modell -> ml_g0, ml_g1 und Log Loss für das PS Modell -> ml_m berechnen\n",
    "        dml_summary['loss_g0'] = np.sqrt(np.mean((true_data_z_a_0['y_a'] - pred_g0[true_data_z_a_0.index])**2)).round(6)\n",
    "        dml_summary['loss_g1'] = np.sqrt(np.mean((true_data_z_a_1['y_a'] - pred_g1[true_data_z_a_1.index])**2)).round(6)\n",
    "        dml_summary['loss_m'] = -np.mean(simdata['z_a'] * np.log(predicted_ps) + (1 - simdata['z_a']) * np.log(1 - predicted_ps)).round(6)\n",
    "\n",
    "        # Ergebnisse speichern\n",
    "        results_tr.append(dml_summary)\n",
    "    return results_tr\n",
    "\n",
    "# Parameter festlegen\n",
    "learner_list = [\n",
    "    {\"ml_g\": LinearRegression(), \"ml_m\": LogisticRegressionCV()},\n",
    "    {\"ml_g\": LinearRegression(), \"ml_m\": RandomForestClassifier()},\n",
    "    {\"ml_g\": LinearRegression(), \"ml_m\": GradientBoostingClassifier()}]\n",
    "scenario_list = ['A', 'E', 'G']\n",
    "\n",
    "# Simulationen parallel ausführen\n",
    "result_trunc = {}\n",
    "for scenario in scenario_list:\n",
    "    result_trunc_scenario = {}\n",
    "    for i_learner, learner in enumerate(learner_list):\n",
    "        result = Parallel(n_jobs=-1)(delayed(run_trunc)(i, ml_g=clone(learner['ml_g']), ml_m=clone(learner['ml_m']), scenario=scenario) for i in range(1000))\n",
    "        result_trunc_scenario[i_learner] = result\n",
    "    result_trunc[scenario] = result_trunc_scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# die absolute Verzerrung berechnen und prüfen, ob der wahre ATTE in den Konfidenzintervallen enthalten ist\n",
    "true_atte = -0.4\n",
    "result_trunc2 = {}\n",
    "for scenario in scenario_list:\n",
    "    result_scnr = result_trunc[scenario]\n",
    "    result_trunc_scnr2 = {}\n",
    "    for i_learner, learner in enumerate(learner_list):\n",
    "        res = result_scnr [i_learner]\n",
    "        combined_dfs_trunc = [pd.concat(inner_list, ignore_index=True) for inner_list in res]\n",
    "        results = pd.concat(combined_dfs_trunc, ignore_index=True)\n",
    "        results['bias'] = results['coef'] - true_atte\n",
    "        results['abs_bias'] = np.abs(results['coef'] - true_atte) \n",
    "        results['in_ci'] = np.where((true_atte >= results['2.5 %']) & (true_atte <= results['97.5 %']), 1, 0)\n",
    "        result_trunc_scnr2[i_learner] = results\n",
    "    result_trunc2[scenario] = result_trunc_scnr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speichern die Ergebnisse in CSV-Dateien\n",
    "for scenario in scenario_list:\n",
    "    result_scnr = result_trunc2[scenario]\n",
    "    results = pd.concat([result_scnr[0], result_scnr[1], result_scnr[2]], ignore_index=True)\n",
    "    results.to_csv(f\"results_trunc_{scenario}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discarding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed für die Reproduzierbarkeit festlegen\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Schätzen die Propensity Score und den ATTE für verschiedene Schwellenwerte, Scenarien und Machine Learning Algorithmen\n",
    "def run_discard(i, ml_g, ml_m, scenario):\n",
    "    # Liste für die Ergebnisse erstellen\n",
    "    result_discard = []\n",
    "    # Seed festlegen\n",
    "    np.random.seed(i)\n",
    "    # Simulationsdaten erstellen\n",
    "    df_data = generate_data(500, scenario)\n",
    "    # wahren Propensity Score speichern\n",
    "    trueps = df_data['z_a_trueps']\n",
    "    # Cross-Fitting\n",
    "    resampling_obj = DoubleMLResampling(n_folds=5, n_rep=1, n_obs=len(df_data), stratify=df_data.z_a)\n",
    "    smpls = resampling_obj.split_samples()\n",
    "\n",
    "    # Propensity Scores schätzen\n",
    "    pscore_est = cross_val_predict(ml_m, df_data.drop(['z_a', 'y_a'], axis=1), df_data.z_a, method='predict_proba', cv=resampling_obj.resampling)[:,1]\n",
    "\n",
    "    for trim_value in [0.001, 0.01, 0.05, 0.1]:\n",
    "        smpls_new, data_trimmed, pscore_trimmed = pscore_discard(df_data, pscore_est, smpls, trim_value)\n",
    "\n",
    "        # DoubleMLData Objekt erstellen\n",
    "        dml_data = DoubleMLData.from_arrays(x=data_trimmed.drop(columns = ['z_a', 'y_a']), y=data_trimmed.y_a, d=data_trimmed.z_a)\n",
    "        # DoubleML Objekt erstellen\n",
    "        dml_obj = DoubleMLIRM(dml_data, ml_g, ml_m, trimming_threshold=1e-12, draw_sample_splitting=False, score=\"ATTE\")\n",
    "        dml_obj.set_sample_splitting(smpls_new)\n",
    "        dml_obj.fit(external_predictions={\"d\":{\"ml_m\": pscore_trimmed}})\n",
    "        dml_summary = dml_obj.summary\n",
    "        dml_summary['scenario'] = scenario\n",
    "        dml_summary['trim_value'] = trim_value\n",
    "        dml_summary['learner'] = str(ml_m)\n",
    "        # Anteil der discarded Beobachtungen\n",
    "        dml_summary['share_trimmed_top'] = ((np.where(pscore_est >= (1-trim_value), 1, 0).sum())/pscore_est.shape[0])*100\n",
    "        dml_summary['share_trimmed_bottom'] = ((np.where(pscore_est <= trim_value, 1, 0).sum())/pscore_est.shape[0])*100\n",
    "        # Anteil der Beobachtungen in der Behandlungsgruppe in der Gesamtstichprobe und in der Stichprobe nach dem Discarding\n",
    "        dml_summary['share_treated_pop'] = ((df_data['z_a']== 1).sum()/df_data.shape[0])*100  \n",
    "        dml_summary[\"share_treated_sample\"] = ((data_trimmed['z_a'] == 1).sum()/data_trimmed.shape[0])*100  \n",
    "        # Anteil der discarded Beobachtungen mit wahren PS   \n",
    "        dml_summary['share_trim_orcl_top'] = ((np.where(trueps >= (1-trim_value), 1, 0).sum())/trueps.shape[0])*100\n",
    "        dml_summary['share_trim_orcl_bottom'] = ((np.where(trueps <= trim_value, 1, 0).sum())/trueps.shape[0])*100 \n",
    "\n",
    "        # Nuisance Loss berechnen\n",
    "        dml_summary['loss_ml_g0'] = dml_obj.nuisance_loss['ml_g0'][0][0]\n",
    "        dml_summary['loss_ml_g1'] = dml_obj.nuisance_loss['ml_g1'][0][0]\n",
    "        dml_summary['loss_ml_m'] = dml_obj.nuisance_loss['ml_m'][0][0]\n",
    "        \n",
    "        # Daten\n",
    "        df_data_trimmed = df_data.loc[data_trimmed.index].reset_index(drop=True)\n",
    "        data_trimmed_z_a_0 = df_data_trimmed[df_data_trimmed['z_a'] == 0]\n",
    "        data_trimmed_z_a_1 = df_data_trimmed[df_data_trimmed['z_a'] == 1]\n",
    "        pred_g0 = dml_obj.predictions['ml_g0'].flatten()\n",
    "        pred_g1 = dml_obj.predictions['ml_g1'].flatten()\n",
    "        predicted_ps = pscore_trimmed.flatten()\n",
    "\n",
    "        # RMSE für das Outcome Modell -> ml_g0, ml_g1  und Log Loss für das PS Modell -> ml_m berechnen\n",
    "        dml_summary['loss_g0'] = np.sqrt(np.mean((data_trimmed_z_a_0['y_a'] - pred_g0[data_trimmed_z_a_0.index])**2)).round(6)\n",
    "        dml_summary['loss_g1'] = np.sqrt(np.mean((data_trimmed_z_a_1['y_a'] - pred_g1[data_trimmed_z_a_1.index])**2)).round(6)\n",
    "        dml_summary['loss_m'] = -np.mean(df_data_trimmed['z_a'] * np.log(predicted_ps) + (1 - df_data_trimmed['z_a']) * np.log(1 - predicted_ps)).round(6)    \n",
    "\n",
    "        # Ergebnisse speichern\n",
    "        result_discard.append(dml_summary)    \n",
    "    return result_discard\n",
    "\n",
    "# Parameter festlegen\n",
    "learner_list = [\n",
    "    {\"ml_g\": LinearRegression(), \"ml_m\": LogisticRegressionCV()},\n",
    "    {\"ml_g\": LinearRegression(), \"ml_m\": RandomForestClassifier()},\n",
    "    {\"ml_g\": LinearRegression(), \"ml_m\": GradientBoostingClassifier()}]\n",
    "scenario_list = ['A', 'E', 'G']\n",
    "\n",
    "# Simulationen parallel ausführen\n",
    "result_discard = {}\n",
    "for scenario in scenario_list:\n",
    "    result_discard_scnr = {}\n",
    "    for i_learner, learner in enumerate(learner_list):\n",
    "        result = Parallel(n_jobs=-1)(delayed(run_discard)(i, ml_g=clone(learner['ml_g']), ml_m=clone(learner['ml_m']), scenario=scenario) for i in range(1000))\n",
    "        result_discard_scnr[i_learner] = result\n",
    "    result_discard[scenario] = result_discard_scnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Die absolute Verzerrung berechnen und prüfen, ob der wahre ATTE in den Konfidenzintervallen enthalten ist\n",
    "true_atte = -0.4\n",
    "result_discard2 = {}\n",
    "for scenario in scenario_list:\n",
    "    result_scnr = result_discard[scenario]\n",
    "    result_discard_scnr2 = {}\n",
    "    for i_learner, learner in enumerate(learner_list):\n",
    "        res = result_scnr[i_learner]\n",
    "        combined_dfs_discard = [pd.concat(inner_list, ignore_index=True) for inner_list in res]\n",
    "        results_discard = pd.concat(combined_dfs_discard, ignore_index=True)\n",
    "        results_discard['bias'] = results_discard['coef'] - true_atte\n",
    "        results_discard['abs_bias'] = np.abs(results_discard['coef'] - true_atte) \n",
    "        results_discard['in_ci'] = np.where((true_atte >= results_discard['2.5 %']) & (true_atte <= results_discard['97.5 %']), 1, 0)\n",
    "        result_discard_scnr2[i_learner] = results_discard\n",
    "    result_discard2[scenario] = result_discard_scnr2\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speichern die Ergebnisse in CSV-Dateien\n",
    "for scenario in scenario_list:\n",
    "    result_scnr = result_discard2[scenario]\n",
    "    results = pd.concat([result_scnr[0], result_scnr[1], result_scnr[2]], ignore_index=True)\n",
    "    results.to_csv(f\"results_disc_{scenario}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
