{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schätzung kausaler Effekte mittels DoubleML unter Verwendung linearer Regression als Outcome-Modell und logistischer Regression, Random Forest und Gradient Boosting als Propensity Score-Modell und schauen, welche Auswirkungen die Discarding und Truncation Strategie auf die Schätzleistung von DoubleML haben. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notwendige Bibliotheken und Funktionen importieren\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, os\n",
    "# hier den Pfad zu der Funktion pscore_discard aus functions.py einfügen\n",
    "sys.path.append(os.path.abspath(\".\"))\n",
    "from DGP import propensity_eq, potential_outcome_eq, make_irm_data\n",
    "from functions import pscore_discard\n",
    "from doubleml import DoubleMLIRM, DoubleMLData\n",
    "from doubleml.utils.resampling import DoubleMLResampling\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.base import clone\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "# Alle Warnungen unterdrücken\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed für die Reproduzierbarkeit der Simulationen festlegen\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Funktion für die Parallelisierung der Simulationen erstellen\n",
    "def run_truncation(i, n_obs, R2_d, R2_y, true_theta, ml_g, ml_m, score):\n",
    "    # Leere Liste für die Ergebnisse erstellen\n",
    "    results = []\n",
    "    # Seed für jede Simulation festlegen\n",
    "    np.random.seed(i)\n",
    "    # Daten generieren\n",
    "    df_dict = make_irm_data(n_obs=n_obs, dim_x=dim_x, theta=true_theta, R2_d=R2_d, R2_y=R2_y)\n",
    "    # DoubleMLData Objekt erstellen\n",
    "    data = DoubleMLData(df_dict['df'], 'Y', 'D', ['x_0', 'x_1'])\n",
    "    orcl_df = df_dict['df_orcl']\n",
    "    # Wahren Propensity Score speichern\n",
    "    trueps = df_dict['df_orcl']['ps']\n",
    "    for trim_value in [0.001, 0.01, 0.05, 0.1]:\n",
    "        # DoubleML Objekt erstellen\n",
    "        dml_obj = DoubleMLIRM(data, ml_g, ml_m, trimming_threshold=trim_value, score=score)\n",
    "        dml_obj.fit()\n",
    "        # geschätzte Propensity Scores speichern\n",
    "        predicted_ps = dml_obj.predictions['ml_m'].flatten()\n",
    "        dml_summary = dml_obj.summary\n",
    "        dml_summary['trim_value'] = trim_value\n",
    "        dml_summary['learner'] = str(ml_g) + \",\" + str(ml_m)\n",
    "        # Anteil der Beobachtungen, die auf der oberen Grenze trunkiert wurden\n",
    "        dml_summary['share_trimmed_top'] = (((predicted_ps == (1-trim_value)).sum())/n_obs)*100 \n",
    "        # Anteil der Beobachtungen, die auf der unteren Grenze trunkiert wurden\n",
    "        dml_summary['share_trimmed_bottom'] = (((predicted_ps == trim_value).sum())/n_obs)*100 \n",
    "        # Anteil der behandelten Beobachtungen für Population und angepasste Stichprobe identisch, da keine Beobachtungen entfernt wurden\n",
    "        dml_summary['share_treated'] = (((df_dict['df']['D'] == 1).sum())/n_obs)*100 \n",
    "        # Anteil der trunkierten Beobachtungen mit wahren PS   \n",
    "        dml_summary['share_trim_orcl_top'] = ((np.where(trueps >= (1-trim_value), 1, 0).sum())/trueps.shape[0])*100\n",
    "        dml_summary['share_trim_orcl_bottom'] = ((np.where(trueps <= trim_value, 1, 0).sum())/trueps.shape[0])*100    \n",
    "\n",
    "        #Nuisance Loss berechnen\n",
    "        dml_summary['loss_ml_g0'] = dml_obj.nuisance_loss['ml_g0'][0][0]\n",
    "        dml_summary['loss_ml_g1'] = dml_obj.nuisance_loss['ml_g1'][0][0]\n",
    "        dml_summary['loss_ml_m'] = dml_obj.nuisance_loss['ml_m'][0][0]\n",
    "\n",
    "       # Daten\n",
    "        orcl_data_g0 = orcl_df[orcl_df['D'] == 0]\n",
    "        orcl_data_g1 = orcl_df[orcl_df['D'] == 1]\n",
    "\n",
    "        # RMSE für das Outcome Modell -> ml_g0, ml_g1  und Log Loss für die Propensity Score-Modell -> ml_m berechnen\n",
    "        dml_summary['loss_g0'] = np.sqrt(np.mean((orcl_data_g0['Y'] - dml_obj.predictions['ml_g0'].flatten()[orcl_data_g0.index])**2)).round(6)\n",
    "        dml_summary['loss_g1'] = np.sqrt(np.mean((orcl_data_g1['Y'] - dml_obj.predictions['ml_g1'].flatten()[orcl_data_g1.index])**2)).round(6)\n",
    "        dml_summary['loss_m'] = -np.mean(orcl_df['D'] * np.log(predicted_ps) + (1 - orcl_df['D']) * np.log(1 - predicted_ps)).round(6)\n",
    "\n",
    "        # Oracle ATTE/ATE berechnen\n",
    "        if score == \"ATTE\":\n",
    "            dml_summary[\"oracle\"] = np.mean(orcl_df.loc[orcl_df.D==1, \"Y_1\"] - orcl_df.loc[orcl_df.D==1, \"Y_0\"]) \n",
    "        else: # ATE\n",
    "            dml_summary[\"oracle\"] = np.mean(orcl_df.Y_1 - orcl_df.Y_0) \n",
    "        \n",
    "        # Ergebnisse speichern\n",
    "        results.append(dml_summary)    \n",
    "    return results\n",
    "\n",
    "# Parameter für die Simulationen festlegen\n",
    "R2_d = 0.8\n",
    "R2_y = 0.8\n",
    "dim_x = 2\n",
    "n_obs = 1000\n",
    "num_repetitions = 1000\n",
    "true_theta = 0\n",
    "learner_list = [\n",
    "    {\"ml_g\": LinearRegression(), \"ml_m\": LogisticRegressionCV()},\n",
    "    {\"ml_g\": LinearRegression(), \"ml_m\": RandomForestClassifier()},\n",
    "    {\"ml_g\": LinearRegression(), \"ml_m\": GradientBoostingClassifier()}]\n",
    "score_list = [\"ATTE\", \"ATE\"] \n",
    "\n",
    "trunc_dict_atte1 = {}\n",
    "trunc_dict_ate1 = {}\n",
    "\n",
    "# Simulationen parallel durchführen\n",
    "for i_learners, learners in enumerate(learner_list):\n",
    "    for score in score_list:\n",
    "        results = Parallel(n_jobs=-1)(delayed(run_truncation)(i, n_obs, R2_d, R2_y, true_theta, ml_g=clone(learners['ml_g']), ml_m=clone(learners['ml_m']), score=score) for i in range(num_repetitions))\n",
    "        if score == \"ATTE\":\n",
    "            trunc_dict_atte1[i_learners] = results\n",
    "        else:\n",
    "            trunc_dict_ate1[i_learners] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolute Verzerrung berechnen und prüfen, ob der Referenzwert innerhalb des Konfidenzintervalls liegt\n",
    "trunc_dict_atte2 = {}\n",
    "trunc_dict_ate2 = {}\n",
    "\n",
    "for i_learners, learners in enumerate(learner_list):\n",
    "    for res in [trunc_dict_atte1,  trunc_dict_ate1]:\n",
    "        dfs = res[i_learners]\n",
    "        # Kombinieren die DataFrames innerhalb jeder inneren Liste und dann zu einem einzigen DataFrame speichern\n",
    "        combined_dfs = [pd.concat(inner_list, ignore_index=True) for inner_list in dfs]\n",
    "        result_df = pd.concat(combined_dfs, ignore_index=True)\n",
    "        \n",
    "        # Absolute Verzerrung berechnen\n",
    "        if res is trunc_dict_atte1:\n",
    "            result_df[\"bias\"] = result_df[\"coef\"] - result_df[\"oracle\"]\n",
    "            result_df[\"abs_bias\"] = np.abs(result_df[\"coef\"] - result_df[\"oracle\"])\n",
    "            # Prüfen, ob der Oracle Wert innerhalb des Konfidenzintervalls liegt\n",
    "            result_df[\"in_ci\"] = np.where((result_df[\"oracle\"] >= result_df[\"2.5 %\"]) & (result_df[\"oracle\"] <= result_df[\"97.5 %\"]), 1, 0) \n",
    "        else:\n",
    "            result_df[\"bias\"] = result_df[\"coef\"] - true_theta\n",
    "            result_df['abs_bias'] = np.abs(result_df[\"coef\"] - true_theta)\n",
    "            # Prüfen, ob der wahre Wert innerhalb des Konfidenzintervalls liegt\n",
    "            result_df[\"in_ci\"] = np.where((true_theta >= result_df[\"2.5 %\"]) & (true_theta <= result_df[\"97.5 %\"]), 1, 0)\n",
    "              \n",
    "        # Ergebnisse speichern\n",
    "        if res is trunc_dict_atte1:\n",
    "            trunc_dict_atte2[i_learners] = result_df\n",
    "        else:\n",
    "            trunc_dict_ate2[i_learners] = result_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mittlere absolute Verzerrung, Varianz des Schätzers, MSE und Coverage berechnen\n",
    "trunc_dict_atte3 = {}\n",
    "trunc_dict_ate3 = {}\n",
    "\n",
    "for i_learners, learners in enumerate(learner_list):\n",
    "    for res in [trunc_dict_atte2,  trunc_dict_ate2]:\n",
    "        result_df = res[i_learners]\n",
    "\n",
    "        for col in result_df.trim_value.unique():\n",
    "            result_df.loc[result_df.trim_value == col, \"mean_abs_bias\"] = result_df.loc[result_df.trim_value == col, \"abs_bias\"].mean()\n",
    "            result_df.loc[result_df.trim_value == col, \"var\"] = result_df.loc[result_df.trim_value == col, \"coef\"].var()\n",
    "            result_df.loc[result_df.trim_value == col, \"MSE\"] = (result_df.loc[result_df.trim_value == col, \"bias\"].pow(2)).mean() \n",
    "            result_df.loc[result_df.trim_value == col, \"coverage\"] = (result_df.loc[result_df.trim_value == col, \"in_ci\"].sum()) / result_df.loc[result_df.trim_value == col].shape[0]\n",
    "\n",
    "        if res is trunc_dict_atte2:\n",
    "            trunc_dict_atte3[i_learners] = result_df\n",
    "        else:\n",
    "            trunc_dict_ate3[i_learners] = result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ergebnisse als csv Datei speichern\n",
    "trunc_atte = pd.concat(trunc_dict_atte3, ignore_index=True)\n",
    "trunc_ate = pd.concat(trunc_dict_ate3, ignore_index=True)\n",
    "trunc_atte.to_csv(\"trunc_atte_linear+logit_rf_gb.csv\", index=False)\n",
    "trunc_ate.to_csv(\"trunc_ate_linear+logit_rf_gb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discarding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed für die Reproduzierbarkeit der Simulationen festlegen\n",
    "np.random.seed(1234)\n",
    "# Funktion für die Parallelisierung der Simulationen\n",
    "def run_discarding(i, n_obs, R2_d, R2_y, true_theta, ml_g, ml_m, score):\n",
    "    # Leere Liste für die Ergebnisse erstellen\n",
    "    results_discard = []\n",
    "    # Seed für jede Simulation festlegen\n",
    "    np.random.seed(i)\n",
    "    # Daten generieren\n",
    "    df_dict = make_irm_data(n_obs=n_obs, dim_x=dim_x, theta=true_theta, R2_d=R2_d, R2_y=R2_y)\n",
    "    data = df_dict['df']\n",
    "    # Wahren Propensity Scores speichern\n",
    "    trueps = df_dict['df_orcl']['ps']\n",
    "    # Cross-Fitting\n",
    "    resampling_obj = DoubleMLResampling(n_folds=5, n_rep=1, n_obs=n_obs, stratify=data.D)\n",
    "    smpls = resampling_obj.split_samples()\n",
    "\n",
    "    # Propensity Scores schätzen\n",
    "    pscore_est = cross_val_predict(ml_m, data.drop(columns=[\"Y\", \"D\"]), data.D, method='predict_proba', cv=resampling_obj.resampling)[:,1]\n",
    "    \n",
    "    for trim_value in [0.001, 0.01, 0.05, 0.1]:\n",
    "        # Discarden der Beobachtungen mit Propensity Scores unterhalb oder oberhalb des Schwellenwerts\n",
    "        smpls_new, data_trimmed, pscore_trimmed = pscore_discard(data, pscore_est, smpls, trim_value)\n",
    "        \n",
    "        # Erstellen DML Daten Objekt\n",
    "        dml_data = DoubleMLData.from_arrays(x=data_trimmed.drop(columns=[\"Y\", \"D\"]),\n",
    "                                            y=data_trimmed[\"Y\"],\n",
    "                                            d=data_trimmed[\"D\"])\n",
    "        # DoubleML Objekt erstellen\n",
    "        dml_obj = DoubleMLIRM(dml_data, ml_g, ml_m, trimming_threshold=1e-12, draw_sample_splitting = False, score=score) \n",
    "        dml_obj.set_sample_splitting(smpls_new)\n",
    "        dml_obj.fit(external_predictions={\"d\":{\"ml_m\": pscore_trimmed}})\n",
    "        dml_summary = dml_obj.summary        \n",
    "        dml_summary[\"trim_value\"] = trim_value \n",
    "        dml_summary['learner'] = str(ml_g) + \",\" + str(ml_m)\n",
    "        # Anteil der behandelten Beobachtungen in der Population\n",
    "        dml_summary['share_treated_pop'] = ((data['D']== 1).sum()/data.shape[0])*100   \n",
    "        # Anteil der behandelten Beobachtungen in der Stichprobe nach dem Discarding\n",
    "        dml_summary[\"share_treated_sample\"] = (((data_trimmed['D'] == 1).sum())/data_trimmed.shape[0])*100  \n",
    "        # Anteil der Beobachtungen, die oberhalb (1-trim_value) Schwellenwerts discarded wurden\n",
    "        dml_summary['share_trimmed_top'] = ((np.where(pscore_est >= (1-trim_value), 1, 0).sum())/pscore_est.shape[0])*100 \n",
    "        # Anteil der Beobachtungen, die unterhalb des Schwellenwerts discarded wurden\n",
    "        dml_summary['share_trimmed_bottom'] = ((np.where(pscore_est <= trim_value, 1, 0).sum())/pscore_est.shape[0])*100 \n",
    "        # Anteil der discarded Beobachtungen mit wahren Propensity Scores   \n",
    "        dml_summary['share_trim_orcl_top'] = ((np.where(trueps >= (1-trim_value), 1, 0).sum())/trueps.shape[0])*100\n",
    "        dml_summary['share_trim_orcl_bottom'] = ((np.where(trueps <= trim_value, 1, 0).sum())/trueps.shape[0])*100 \n",
    "        \n",
    "        # Subpopulation nach dem Discarding\n",
    "        df_orcl = df_dict[\"df_orcl\"].loc[data_trimmed.index].reset_index(drop=True)\n",
    "        predicted_ps = pscore_trimmed.flatten()\n",
    "\n",
    "        # Log Loss berechnen\n",
    "        dml_summary['loss_ml_g0'] = dml_obj.nuisance_loss['ml_g0'][0][0]\n",
    "        dml_summary['loss_ml_g1'] = dml_obj.nuisance_loss['ml_g1'][0][0]\n",
    "        dml_summary['loss_ml_m'] = dml_obj.nuisance_loss['ml_m'][0][0]\n",
    "\n",
    "        # RMSE für das Outcome Modell -> ml_g0, ml_g1  und Log Loss für das Propensity Score-Modell -> ml_m berechnen\n",
    "        orcl_data_g0 = df_orcl[df_orcl['D'] == 0]\n",
    "        orcl_data_g1 = df_orcl[df_orcl['D'] == 1]\n",
    "        dml_summary['loss_g0'] = np.sqrt(np.mean((orcl_data_g0['Y'] - dml_obj.predictions['ml_g0'].flatten()[orcl_data_g0.index])**2)).round(6)\n",
    "        dml_summary['loss_g1'] = np.sqrt(np.mean((orcl_data_g1['Y'] - dml_obj.predictions['ml_g1'].flatten()[orcl_data_g1.index])**2)).round(6)\n",
    "        dml_summary['loss_m'] = -np.mean(df_orcl['D'] * np.log(predicted_ps) + (1 - df_orcl['D']) * np.log(1 - predicted_ps)).round(6)\n",
    "\n",
    "        # Oracle ATTE/ATE berechnen\n",
    "        if score == \"ATTE\":\n",
    "            dml_summary[\"oracle\"] = np.mean(df_orcl.loc[df_orcl.D==1,\"Y_1\"] - df_orcl.loc[df_orcl.D==1,\"Y_0\"])\n",
    "        else:\n",
    "            dml_summary[\"oracle\"] = np.mean(df_orcl[\"Y_1\"] - df_orcl[\"Y_0\"])\n",
    "\n",
    "        # Ergebnisse speichern\n",
    "        results_discard.append(dml_summary)\n",
    "    return results_discard\n",
    "\n",
    "# Parameter für die Simulationen\n",
    "R2_d = 0.8\n",
    "R2_y = 0.8\n",
    "dim_x = 2\n",
    "n_obs = 1000\n",
    "num_repetitions = 1000\n",
    "true_theta = 0\n",
    "learner_list = [\n",
    "    {\"ml_g\": LinearRegression(), \"ml_m\": LogisticRegressionCV()},\n",
    "    {\"ml_g\": LinearRegression(), \"ml_m\": RandomForestClassifier()},\n",
    "    {\"ml_g\": LinearRegression(), \"ml_m\": GradientBoostingClassifier()}\n",
    "    ]\n",
    "score_list = [\"ATTE\", \"ATE\"] \n",
    "\n",
    "# Simulationen parallel durchführen\n",
    "disc_dict_atte1 = {}\n",
    "disc_dict_ate1 = {}\n",
    "for i_learners, learners in enumerate(learner_list):\n",
    "    for score in score_list:\n",
    "        results_discard = Parallel(n_jobs=-1)(delayed(run_discarding)(i, n_obs, R2_d, R2_y, true_theta,  ml_g=clone(learners['ml_g']), ml_m=clone(learners['ml_m']), score=score) for i in range(num_repetitions))\n",
    "        if score == \"ATTE\":\n",
    "            disc_dict_atte1[i_learners] = results_discard\n",
    "        else:\n",
    "            disc_dict_ate1[i_learners] = results_discard          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolute Verzerrung berechnen und prüfen, ob der Referenzwert innerhalb des Konfidenzintervalls liegt\n",
    "disc_dict_atte2 = {}\n",
    "disc_dict_ate2 = {}\n",
    "\n",
    "for i_learners, learners in enumerate(learner_list):\n",
    "    for res in [disc_dict_atte1, disc_dict_ate1]:\n",
    "        dfs = res[i_learners]\n",
    "        # Kombinieren die DataFrames innerhalb jeder inneren Liste und dann zu einem einzigen DataFrame speichern\n",
    "        combined_dfs = [pd.concat(inner_list, ignore_index=True) for inner_list in dfs]\n",
    "        result_df = pd.concat(combined_dfs, ignore_index=True)  \n",
    "\n",
    "        if res is disc_dict_atte1:\n",
    "            result_df['bias'] = result_df['coef'] - result_df['oracle']\n",
    "            result_df['abs_bias'] = np.abs(result_df['coef'] - result_df['oracle'])\n",
    "            # Prüfen, ob der Oracle-Wert innerhalb des Konfidenzintervalls liegt\n",
    "            result_df[\"in_ci\"] = np.where((result_df[\"oracle\"] >= result_df[\"2.5 %\"]) & (result_df[\"oracle\"] <= result_df[\"97.5 %\"]), 1, 0)\n",
    "        else:\n",
    "           result_df['bias'] = result_df['coef'] - true_theta\n",
    "           result_df['abs_bias'] = np.abs(result_df['coef'] - true_theta) \n",
    "           # Prüfen, ob der wahre Wert innerhalb des Konfidenzintervalls liegt\n",
    "           result_df[\"in_ci\"] = np.where((true_theta >= result_df[\"2.5 %\"]) & (true_theta <= result_df[\"97.5 %\"]), 1, 0)       \n",
    "        \n",
    "        # Ergebnisse speichern\n",
    "        if res is disc_dict_atte1:\n",
    "            disc_dict_atte2[i_learners] = result_df\n",
    "        else:\n",
    "            disc_dict_ate2[i_learners] = result_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mittlere absolute Verzerrung, Varianz des Schätzers, MSE und Coverage berechnen\n",
    "disc_dict_atte3 = {}\n",
    "disc_dict_ate3 = {}\n",
    "\n",
    "for i_learners, learners in enumerate(learner_list):\n",
    "    for res in [disc_dict_atte2, disc_dict_ate2]:\n",
    "        df = res[i_learners]  \n",
    "        for col in df.trim_value.unique():\n",
    "            df.loc[df.trim_value == col, \"mean_abs_bias\"] = df.loc[df.trim_value == col, \"abs_bias\"].mean() \n",
    "            df.loc[df.trim_value == col, \"var\"] = df.loc[df.trim_value == col, \"coef\"].var()\n",
    "            df.loc[df.trim_value == col, \"MSE\"] = (df.loc[df.trim_value == col, \"bias\"].pow(2)).mean()\n",
    "            df.loc[df.trim_value == col, \"coverage\"] = (df.loc[df.trim_value == col, \"in_ci\"].sum()) / df.loc[df.trim_value == col].shape[0] \n",
    "            \n",
    "        if res is disc_dict_atte2:\n",
    "            disc_dict_atte3[i_learners] = df\n",
    "        else:\n",
    "            disc_dict_ate3[i_learners] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ergebnisse als csv Datei speichern\n",
    "disc_atte = pd.concat(disc_dict_atte3, ignore_index=True)\n",
    "disc_ate = pd.concat(disc_dict_ate3, ignore_index=True)\n",
    "disc_atte.to_csv(\"disc_atte_linear+logit_rf_gb.csv\", index=False)\n",
    "disc_ate.to_csv(\"disc_ate_linear+logit_rf_gb.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
